{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10332355,"sourceType":"datasetVersion","datasetId":6397293}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport timeit\nimport numpy\nfrom numpy.random import seed\nfrom tensorflow.python.framework.random_seed import set_seed\nimport os\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom  functools import reduce\nfrom keras.layers import Dense,Flatten, Dropout\nfrom  keras import Input, Model\nimport keras.backend as K\n\nimport pandas as pd\nfrom ast import literal_eval\nfrom sklearn.model_selection import train_test_split, KFold\nimport numpy as np\nfrom itertools import chain\nimport en_core_web_sm\n\nfrom transformers import AutoConfig, AutoModel,AutoTokenizer\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\nfrom keras.layers import Dense,Flatten, Dropout\nfrom  keras import Input, Model\nimport keras.backend as K\nfrom transformers import TFBertModel\nfrom transformers import TFRobertaModel,  TFAlbertModel, TFXLNetModel\n\nimport tensorflow as tf\nfrom keras import Input, Model\nfrom keras.layers import Dense, Flatten, Layer\nfrom transformers import TFBertModel\n\n\nnlp = en_core_web_sm.load()\n\nseed(1)\nset_seed(2)\nfrom pandas import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:00.686955Z","iopub.execute_input":"2025-01-21T07:13:00.687192Z","iopub.status.idle":"2025-01-21T07:13:09.113628Z","shell.execute_reply.started":"2025-01-21T07:13:00.687172Z","shell.execute_reply":"2025-01-21T07:13:09.112703Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"url='/kaggle/input/toxicspan/CR_full_span_dataset.csv'\nfull_token='/kaggle/input/toxicspan/full_token_list.csv'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.114576Z","iopub.execute_input":"2025-01-21T07:13:09.115318Z","iopub.status.idle":"2025-01-21T07:13:09.118960Z","shell.execute_reply.started":"2025-01-21T07:13:09.115280Z","shell.execute_reply":"2025-01-21T07:13:09.118179Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##test train ratio\ndef generate_test_train_index( length, ratio):\n    rand_array=np.arange(0,length,1)\n    train, test =train_test_split(rand_array,test_size=ratio, shuffle=True )\n    train=numpy.sort(train)\n    test=numpy.sort(test)\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.119717Z","iopub.execute_input":"2025-01-21T07:13:09.119979Z","iopub.status.idle":"2025-01-21T07:13:09.137034Z","shell.execute_reply.started":"2025-01-21T07:13:09.119954Z","shell.execute_reply":"2025-01-21T07:13:09.136401Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# spliting each text (Code Rreview comment) to sentences\ndef split_paragraph_to_sentences(data):\n    base_offset = []\n    sentences = []\n    sentence_spans = []\n    sent_start = []\n\n    print(\"Shape: \", data.shape[0])\n\n    for i in range(len(data)):\n        # print( data['text'][i])\n        text = str(data['text'][i])\n        spans = data['spans'][i]\n        left_ptr = 0\n        right_ptr = 0\n        started = False\n        doc = nlp(text)\n        for sent in doc.sents:\n            start = doc[sent.start].idx\n            end = doc[sent.end - 1].idx + len(doc[sent.end - 1])\n            if end - start <= 1:\n                continue\n            sentences.append(text[start:end])\n            base_offset.append(start)\n            if started == False:\n                started = True\n                sent_start.append(True)\n            else:\n                sent_start.append(False)\n            while left_ptr < len(spans) and spans[left_ptr] < start:\n                left_ptr += 1\n            right_ptr = left_ptr\n            cur_spans = []\n            while right_ptr < len(spans) and spans[right_ptr] < end:\n                cur_spans.append(spans[right_ptr])\n                right_ptr += 1\n            sentence_spans.append(cur_spans)\n            left_ptr = right_ptr\n        if started == False:\n            sentences.append(text)\n            base_offset.append(0)\n            sentence_spans.append(spans)\n            sent_start.append(True)\n    data_processed = pd.DataFrame(\n        data={\"text\": sentences, \"spans\": sentence_spans, \"base_offset\": base_offset, \"sent_start\": sent_start})\n    return data_processed # dataframe -> \n\n#    text                            spans                 base_offset   sent_start\n# 0  \"yeah that sucked, fixed   [10, 11, 12, 13, 14, 15]         0         True\n\n# base_offset: starting index of the sentence in the original text\n# sent_start: Boolean flag indicating if the sentence is the start of a new sentence in the paragraph.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.139251Z","iopub.execute_input":"2025-01-21T07:13:09.139528Z","iopub.status.idle":"2025-01-21T07:13:09.155273Z","shell.execute_reply.started":"2025-01-21T07:13:09.139499Z","shell.execute_reply":"2025-01-21T07:13:09.154648Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# prepare the output toxic/non toxic labels\n# purpose : checking number of toxic characters is greater than or equal to half the length of the token\ndef generate_output(data, tokens, max_length,tokenizer): #((dataframe->'text','spans','base_offset','sent_start'), (input_ids,token_type_ids,attention_mask,special_tokens_mask),70,bert) \n        output = np.zeros((data.shape[0], max_length)) # np.zeros((1, 70))\n        for i in range(data.shape[0]): # data.shape[0]=1\n            isToxic = np.zeros(len(data.text[i])) # len=22\n            # np.zeros(len(\"yeah that sucked, fixed\"))\n            # np.zeros(22)\n            #is_toxic=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0.])\n            for idx in data.spans[i]: # data.spans[0]: [10, 11, 12, 13, 14, 15] \n                isToxic[idx - data.base_offset[i]] = 1 # toxic letter=1  // base_offset[0]=0\n#idx = 10\n#isToxic[10 - 0] = 1\n# isToxic[10]=1\n# array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n#final isToxic=[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]\n\n            # finding special tokens\n            for j in range(max_length): # 0-69\n                if tokens.special_tokens_mask[i][j]: # [0][0] i=dataframe row / j=every row 70 array elements/// check if thele element is special token\n                    continue\n\n                sp_tok=tokenizer.convert_ids_to_tokens(tokens.input_ids[i][j]) \n                # tokenizer.convert_ids_to_tokens converts the token ID back to its string representation\n                # If the token ID 1037 corresponds to the word \"sucked\", then convert_ids_to_tokens will return \"sucked\"\n                \n                ##this portion is added to making some unwanted tokens as 0 (non-toxic)\n                # these tokens are generated for pretrained tokenizers\n                if(sp_tok==\"_\" or sp_tok==\"s\" or sp_tok==\"D\"):\n                    continue\n                if (sp_tok == \"Ġ\" or  sp_tok==\"Ġ,\" or sp_tok==\"Ġ.\" or sp_tok==\"Ġ/\" or sp_tok==\"Ġs\" or sp_tok==\"Ġ[\" or sp_tok==\"Ġy\" or sp_tok==\"Ġd\" or sp_tok==\"Ġ@\" or sp_tok==\"Ġa\"):\n                    continue\n                if (sp_tok==\"ĠI\" or sp_tok==\"Ġ(\" or sp_tok==\"Ġi\" or sp_tok==\"Ġ*\" or sp_tok==\"Ġ_\" or sp_tok==\"Ġ:\" or sp_tok==\"Ġ=\" or sp_tok==\"Ġ-\"):\n                    continue\n                if(sp_tok==\"ĠU\" or sp_tok==\"Ġ#\" or sp_tok==\"ĠD\" or sp_tok==\"ĠA\" or sp_tok==\"ĠS\" or sp_tok==\"Ġ\\\"\" or sp_tok==\"Ġ,\" or sp_tok==\"Ġ>\"):\n                    continue\n                start = tokens.offset_mapping[i][j][0] # i= sentence number // j= token number // 0=start 1=end\n                end = tokens.offset_mapping[i][j][1]\n                cnt = 0\n                for pos in range(start, end):\n                    if isToxic[pos]:\n                        cnt += 1\n# Check if the number of toxic characters (cnt) is greater than or equal to half the length of the token.                       \n                if cnt >= (end - start + 1) // 2:\n                    output[i][j] = 1\n        return output  # returns array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.156558Z","iopub.execute_input":"2025-01-21T07:13:09.156765Z","iopub.status.idle":"2025-01-21T07:13:09.182850Z","shell.execute_reply.started":"2025-01-21T07:13:09.156748Z","shell.execute_reply":"2025-01-21T07:13:09.182255Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"##preprocess each sentence\ndef sentence_preprocessing(data, max_length, tokenizer):    #dataframe,70,bert\n        data_processed = split_paragraph_to_sentences(data) # in: dataframe / return: 'text': [\"yeah that sucked, fixed\"], 'spans': [[10, 11, 12, 13, 14, 15]], 'base_offset': [0], 'sent_start': [True]\n        print(len(data_processed['text'])) # 1\n        input_texts =data_processed['text'] #\"  pd.Series([\"yeah that sucked, fixed\"])\n        input_texts=input_texts.to_numpy() # np.array([\"yeah that sucked, fixed\"]) ::::: converted to numpy array->Performance Optimization,Compatibility,Memory Usage\n        tokens = tokenizer(list(data_processed.text), max_length=max_length, padding=\"max_length\",         \n                           truncation=True, return_offsets_mapping=True, return_special_tokens_mask=True,\n                           return_token_type_ids=True)\n\n# tokens = {\n#     \"input_ids\": [ 101, 2748, 2008, 5638, 1010, 4714, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n    # Each unique word or subword has a specific ID in the tokenizer's vocabulary.\n#     \"token_type_ids\": [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],               \n    # Segment IDs (0 indicates all tokens are in the same segment/sentence)\n#     \"attention_mask\": [ 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],               \n    # Attention mask (1 means the token should be attended to) (1=words 0=padding)\n#     \"special_tokens_mask\": [ 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ],           \n    # Special tokens mask (1 for special tokens like [CLS] , [SEP] and [PAD]) (1 for special tokens, 0 for regular tokens)\n    # \"offset_mapping\": [ (0, 0), (0, 4), (5, 9), (10, 16), (16, 17), (18, 23), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), \n    #                    (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0) ]\n    # every word's start end position\n# }\n\n\n\n        data_input = {\"input_ids\": np.array(tokens.input_ids),\n                      \"token_type_ids\": np.array(tokens.token_type_ids),\n                      \"attention_mask\": np.array(tokens.attention_mask)}\n\n\n# data_input = {\n# \"input_ids\": np.array([[101, 2748, 2008, 5638, 1010, 4714, 102]]),\n# \"token_type_ids\": np.array([[0, 0, 0, 0, 0, 0, 0]]),\n# \"attention_mask\": np.array([[1, 1, 1, 1, 1, 1, 1]])\n# }\n\n        # to check whether contains toxic word\n        data_output = generate_output(data_processed, tokens, max_length,tokenizer) \n# np.array([[0, 0, 0, 1, 0, 0, 0]])  =  ((dataframe->'text','spans','base_offset','sent_start'), (input_ids,token_type_ids,attention_mask,special_tokens_mask),70,bert)   \n\n        inital_weights = 1 - np.array(tokens.special_tokens_mask) # 1-[1,0,0,0,0,0,1]=np.array([[0, 1, 1, 1, 1, 1, 0]])\n        return data_input, input_texts, data_output, inital_weights,tokens\n\n\n# Data Input: {'input_ids': array([[101, 2026, 2001, 2005, 1037, 3946, 102]]),\n#              'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]),\n#              'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}\n\n# Input Texts: ['yeah that sucked, fixed']\n\n# Data Output: [[0, 0, 0, 1, 0, 0, 0]]\n\n# Initial Weights: [[0, 1, 1, 1, 1, 1, 0]]\n\n# Tokens: {'input_ids': [[101, 2026, 2001, 2005, 1037, 3946, 102]],\n#          'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]],\n#          'attention_mask': [[1, 1, 1, 1, 1, 1, 1]],\n#          'special_tokens_mask': [[1, 0, 0, 0, 0, 0, 1]]\n#          \"offset_mapping\": [(0, 0), (0, 4), (5, 9), (10, 16), (16, 17), (18, 23), (0, 0)] \n#}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.183583Z","iopub.execute_input":"2025-01-21T07:13:09.183893Z","iopub.status.idle":"2025-01-21T07:13:09.206171Z","shell.execute_reply.started":"2025-01-21T07:13:09.183864Z","shell.execute_reply":"2025-01-21T07:13:09.205520Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def classify_by_threshold(prediction, thresh):\n  predictions = np.zeros(prediction.shape)\n  for i in range(prediction.shape[0]):\n    for j in range(prediction.shape[1]):\n      if prediction[i,j]>=thresh:\n        predictions[i,j]=1\n  return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.206900Z","iopub.execute_input":"2025-01-21T07:13:09.207107Z","iopub.status.idle":"2025-01-21T07:13:09.227397Z","shell.execute_reply.started":"2025-01-21T07:13:09.207079Z","shell.execute_reply":"2025-01-21T07:13:09.226787Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"## this function is for text encoding which provides the explainable output\ndef convert_to_original_text_with_toxicity(encoded_text,pred,tokenizer):\n  ##list_decoded_text_toxicity = []\n  only_decoded_text = tokenizer.convert_ids_to_tokens(encoded_text)\n  decode = [tokenizer.convert_tokens_to_string([i]) for i in tokenizer.convert_ids_to_tokens(encoded_text)]\n  decoded_new = []\n  prev = 0\n\n  if (len(pred) == 0):\n    decoded_new = decode\n  else:\n    #if (pred[0] == 0):\n      #decoded_new.append(['<toxic>'])\n    for i in range(0, len(pred)):\n      pred_val = int(pred[i])\n\n      ##for handling exception\n      if (pred_val >= len(decode)):\n        print(\"Missing here\", only_decoded_text, pred)\n        break\n      if (pred_val > prev and prev > 0):\n        decoded_new.append(['</toxic>'])\n      decoded_new.append(decode[prev:pred_val])\n\n      if (pred_val - prev > 0):\n        decoded_new.append(['<toxic>'])\n\n      decoded_new.append([decode[pred_val]])\n\n      #decoded_new.append([decode[pred_val]])\n      # if(pred_val - prev >0): decoded_new.append(['</toxic>'])\n      prev = pred_val + 1\n    decoded_new.append(['</toxic>'])\n    decoded_new.append(decode[prev:len(decode)])\n\n  decoded_new = list(chain.from_iterable(decoded_new))\n  return decoded_new\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.228154Z","iopub.execute_input":"2025-01-21T07:13:09.228342Z","iopub.status.idle":"2025-01-21T07:13:09.245126Z","shell.execute_reply.started":"2025-01-21T07:13:09.228326Z","shell.execute_reply":"2025-01-21T07:13:09.244496Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## if we want to use BIO for prediction\n# we do not use it in our experiment\ndef classify_bio_output(prediction):\n  num_samples = prediction.shape[0]*prediction.shape[1]\n  predictions = np.zeros((prediction.shape[0],prediction.shape[1]))\n  for i in range(prediction.shape[0]):\n    for j in range(prediction.shape[1]):\n      if prediction[i,j] != 0:\n        predictions[i,j] = 1\n  return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.245900Z","iopub.execute_input":"2025-01-21T07:13:09.246149Z","iopub.status.idle":"2025-01-21T07:13:09.266238Z","shell.execute_reply.started":"2025-01-21T07:13:09.246130Z","shell.execute_reply":"2025-01-21T07:13:09.265466Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# class ReshapeLayer(Layer):\n#     def __init__(self, target_shape, **kwargs):\n#         super(ReshapeLayer, self).__init__(**kwargs)\n#         self.target_shape = target_shape\n\n#     def call(self, inputs):\n#         return tf.reshape(inputs, self.target_shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.266942Z","iopub.execute_input":"2025-01-21T07:13:09.267127Z","iopub.status.idle":"2025-01-21T07:13:09.280781Z","shell.execute_reply.started":"2025-01-21T07:13:09.267110Z","shell.execute_reply":"2025-01-21T07:13:09.280203Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.layers import Layer, Input, Dense, Conv1D, GlobalMaxPooling1D, Dropout\n# from tensorflow.keras.models import Model\n# from transformers import TFBertModel, BertConfig\n\n# class BertLayer(Layer):\n#     def __init__(self, model_name, training=True, **kwargs):\n#         super(BertLayer, self).__init__(**kwargs)\n#         self.training = training\n#         config = BertConfig()  # Create a configuration with default settings\n#         self.bert = TFBertModel(config)\n\n#     def call(self, inputs, training=None):\n#         input_ids = inputs[\"input_ids\"]\n#         token_type_ids = inputs[\"token_type_ids\"]\n#         attention_mask = inputs[\"attention_mask\"]\n\n#         outputs = self.bert(\n#             input_ids=input_ids,\n#             token_type_ids=token_type_ids,\n#             attention_mask=attention_mask,\n#             training=self.training,\n#         )\n#         return outputs.last_hidden_state  # Return embeddings (last hidden state)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.281614Z","iopub.execute_input":"2025-01-21T07:13:09.281813Z","iopub.status.idle":"2025-01-21T07:13:09.295775Z","shell.execute_reply.started":"2025-01-21T07:13:09.281796Z","shell.execute_reply":"2025-01-21T07:13:09.295170Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# def get_bert(training, max_length=70):\n#     # Define Keras inputs\n#     input_ids = Input(shape=(max_length,), name=\"input_ids\", dtype=\"int32\")\n#     token_type_ids = Input(shape=(max_length,), name=\"token_type_ids\", dtype=\"int32\")\n#     attention_mask = Input(shape=(max_length,), name=\"attention_mask\", dtype=\"int32\")\n\n#     # BERT embeddings with defined embedding dimension\n#     embedding_dim = 768  # Default for BERT-base\n#     bert_outputs = BertLayer(model_name=\"bert-base-uncased\", training=training)(\n#         {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n#     )\n#     bert_outputs = ReshapeLayer((-1, max_length, embedding_dim))(bert_outputs)\n#     print(\"BERT output shape:\", bert_outputs.shape)\n\n#     # Sequential CNN layers for feature extraction\n#     conv_1 = Conv1D(filters=512, kernel_size=5, activation=\"relu\")(bert_outputs)\n#     conv_1 = Dropout(0.5)(conv_1)\n#     print(\"Conv1 output shape:\", conv_1.shape)\n\n#     conv_2 = Conv1D(filters=256, kernel_size=5, activation=\"relu\")(conv_1)\n#     conv_2 = Dropout(0.5)(conv_2)\n#     print(\"Conv2 output shape:\", conv_2.shape)\n\n#     conv_3 = Conv1D(filters=128, kernel_size=3, activation=\"relu\")(conv_2)\n#     conv_3 = Dropout(0.5)(conv_3)\n#     print(\"Conv3 output shape:\", conv_3.shape)\n\n#     conv_4 = Conv1D(filters=64, kernel_size=3, activation=\"relu\")(conv_3)\n#     conv_4 = Dropout(0.5)(conv_4)\n#     pooled_4 = GlobalMaxPooling1D()(conv_4)\n#     print(\"Pooled output shape:\", pooled_4.shape)\n\n#     # Dense classification head with additional layers and dropout\n#     dense_layer = Dense(128, activation=\"relu\")(pooled_4)\n#     dense_layer = Dropout(0.5)(dense_layer)\n#     dense_layer = Dense(64, activation=\"relu\")(dense_layer)\n#     dense_layer = Dropout(0.5)(dense_layer)\n#     outputs = Dense(1, activation=\"sigmoid\")(dense_layer)\n\n#     # Define the model\n#     return Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs)\n\n\n\n\n# def get_roberta(training, max_length=70):\n#   input_ids = Input(shape=(max_length,),name=\"input_ids\",dtype=\"int32\")\n#   token_type_ids = Input(shape=(max_length,),name=\"token_type_ids\")\n#   attention_mask = Input(shape=(max_length,),name=\"attention_mask\")\n    \n\n#   # Convert KerasTensors to Tensors \n#   input_ids = tf.convert_to_tensor(input_ids)\n#   token_type_ids = tf.convert_to_tensor(token_type_ids) \n#   attention_mask = tf.convert_to_tensor(attention_mask)\n    \n#   encoder = TFRobertaModel.from_pretrained(\"roberta-base\")\n\n#   embeddings = encoder({\"input_ids\":input_ids,\"token_type_ids\":token_type_ids,\"attention_mask\":attention_mask}, training=training)[0]\n\n#   dense_layer2=Dense(1,activation='sigmoid') (embeddings, training=training)\n#   outputs = Flatten()(dense_layer2)\n#   return Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.296702Z","iopub.execute_input":"2025-01-21T07:13:09.297007Z","iopub.status.idle":"2025-01-21T07:13:09.316229Z","shell.execute_reply.started":"2025-01-21T07:13:09.296978Z","shell.execute_reply":"2025-01-21T07:13:09.315640Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Layer\nfrom transformers import TFRobertaModel\n\nclass MyLayer(Layer):\n    def __init__(self, **kwargs):\n        super(MyLayer, self).__init__(**kwargs)\n        self.dense_layer = Dense(1, activation='sigmoid')\n\n    def build(self, input_shape):\n        self.encoder = TFRobertaModel.from_pretrained(\"roberta-base\")\n        super(MyLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        input_ids, token_type_ids, attention_mask = inputs\n        # Convert token_type_ids to int32\n        token_type_ids = tf.cast(token_type_ids, dtype=tf.int32)\n        embeddings = self.encoder({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}, training=True)[0]\n        outputs = self.dense_layer(embeddings)\n        outputs = Flatten()(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], 1)  # Assuming the input shape is (batch_size, max_length)\n\ndef get_roberta(training, max_length=70):\n    input_ids = Input(shape=(max_length,), name=\"input_ids\", dtype=\"int32\")\n    token_type_ids = Input(shape=(max_length,), name=\"token_type_ids\")\n    attention_mask = Input(shape=(max_length,), name=\"attention_mask\")\n    \n    outputs = MyLayer()([input_ids, token_type_ids, attention_mask])\n    return tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.318960Z","iopub.execute_input":"2025-01-21T07:13:09.319179Z","iopub.status.idle":"2025-01-21T07:13:09.344091Z","shell.execute_reply.started":"2025-01-21T07:13:09.319151Z","shell.execute_reply":"2025-01-21T07:13:09.343213Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np\n\ndef offset_precision_recall_fscore(gold, predictions):\n    num_correct = len(set(predictions).intersection(set(gold)))\n    num_predicted = len(set(predictions))\n    correct_span_length = len(set(gold))\n\n    if num_predicted == 0:\n        if correct_span_length == 0:\n            precision = 1  # non-toxic instance correctly predicted\n        else:\n            precision = 0\n    else:\n        precision = num_correct / num_predicted\n\n    if correct_span_length == 0:\n        if num_predicted == 0:\n            recall = 1  # non-toxic correctly predicted\n            f1 = 1  # non-toxic correctly predicted\n        else:\n            recall = 0\n            f1 = 0\n    else:\n        recall = num_correct / correct_span_length\n        f1 = (2 * num_correct) / (correct_span_length + num_predicted)\n    \n    return precision, recall, f1\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.345214Z","iopub.execute_input":"2025-01-21T07:13:09.345413Z","iopub.status.idle":"2025-01-21T07:13:09.353787Z","shell.execute_reply.started":"2025-01-21T07:13:09.345395Z","shell.execute_reply":"2025-01-21T07:13:09.353041Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def compute_performance(ground_truth, predicted_output, test_texts, tokenizer, maxlen, error_analysis=False):\n    instance_f1_0 = []\n    instance_prec_0 = []\n    instance_recall_0 = []\n    instance_f1_1 = []\n    instance_prec_1 = []\n    instance_recall_1 = []\n    retro_output = []\n\n    for i in range(len(ground_truth)):\n        pred_offset = []\n        test_base_offset = []\n\n        for j in range(maxlen):\n            if ground_truth[i][j] == 1:\n                test_base_offset.append(j)  # ground truth offset\n            if predicted_output[i][0] >= 0.5:  # Assuming threshold of 0.5 for binary classification\n                pred_offset.append(j)  # token offset for prediction\n\n        if len(test_base_offset) == 0:  # non-toxic instance\n            precision_0, recall_0, f1_0 = offset_precision_recall_fscore(test_base_offset, pred_offset)\n            instance_prec_0.append(precision_0)\n            instance_recall_0.append(recall_0)\n            instance_f1_0.append(f1_0)\n        else:  # toxic instances\n            precision_1, recall_1, f1_1 = offset_precision_recall_fscore(test_base_offset, pred_offset)\n            instance_prec_1.append(precision_1)\n            instance_recall_1.append(recall_1)\n            instance_f1_1.append(f1_1)\n        \n        if error_analysis:\n            gold = '[' + ','.join(str(v) for v in test_base_offset) + ']'\n            predicted = '[' + ','.join(str(w) for w in pred_offset) + ']'\n            original_text = test_texts[i]\n\n            list_decoded_text_toxicity = []\n            only_decoded_text = []\n\n            if len(original_text) == 0:\n                only_decoded_text = []\n            else:\n                encoded_text = tokenizer.encode(original_text)\n                only_decoded_text = tokenizer.convert_ids_to_tokens(encoded_text)\n                decoded_actual_text = [tokenizer.convert_tokens_to_string([i]) for i in tokenizer.convert_ids_to_tokens(encoded_text)]\n\n                if len(pred_offset) == 0:\n                    list_decoded_text_toxicity = decoded_actual_text\n                else:\n                    list_decoded_text_toxicity = convert_to_original_text_with_toxicity(encoded_text, pred_offset, tokenizer)\n\n                only_decoded_text_str = ' '.join(str(x) for x in only_decoded_text)\n                decoded_text_toxicity_to_string = ' '.join(str(y) for y in list_decoded_text_toxicity)\n                list_decoded_text_toxicity_to_string = [decoded_text_toxicity_to_string]\n\n            retro_output.append([original_text, list_decoded_text_toxicity_to_string, gold, predicted])\n\n    avg_precision_0 = np.mean(np.array(instance_prec_0))\n    avg_recall_0 = np.mean(np.array(instance_recall_0))\n    avg_f1_0 = np.mean(np.array(instance_f1_0))\n\n    avg_precision_1 = np.mean(np.array(instance_prec_1))\n    avg_recall_1 = np.mean(np.array(instance_recall_1))\n    avg_f1_1 = np.mean(np.array(instance_f1_1))\n\n    return avg_precision_0, avg_recall_0, avg_f1_0, avg_precision_1, avg_recall_1, avg_f1_1, retro_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.354792Z","iopub.execute_input":"2025-01-21T07:13:09.355105Z","iopub.status.idle":"2025-01-21T07:13:09.381534Z","shell.execute_reply.started":"2025-01-21T07:13:09.355076Z","shell.execute_reply":"2025-01-21T07:13:09.380938Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##collecting the pretrained tokenizer\ndef tokenizer_collect(Tokenizer):\n    Tokenizer=Tokenizer.lower()\n    if (Tokenizer == 'bert'):\n        return AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    elif (Tokenizer == 'roberta'):\n        return AutoTokenizer.from_pretrained(\"roberta-base\",add_prefix_space=True)\n    elif (Tokenizer == 'dbert'):\n        return AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    elif (Tokenizer == 'albert'):\n        return AutoTokenizer.from_pretrained(\"albert-base-v2\")\n    elif (Tokenizer == 'deberta'):\n        return AutoTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n    elif (Tokenizer == 'xlnet'):\n        return AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n    else:\n        print(\"Wrong tokenizer selected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.382312Z","iopub.execute_input":"2025-01-21T07:13:09.382627Z","iopub.status.idle":"2025-01-21T07:13:09.408198Z","shell.execute_reply.started":"2025-01-21T07:13:09.382598Z","shell.execute_reply":"2025-01-21T07:13:09.407601Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n# def binary_loss(y_true, y_pred):\n#   loss = -1 * y_true * K.log( K.clip(y_pred + K.epsilon(), 0, 1.0) )\n#   loss += -1 * (1-y_true) * K.log( K.clip(1-y_pred+K.epsilon(), 0, 1.0) )\n#   return loss\n\n# import tensorflow as tf\n# from tensorflow.keras import backend as K\n\n# def binary_loss(y_true, y_pred):\n#     # Clip the predictions to avoid log(0) errors\n#     y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0 - K.epsilon())\n#     # Compute binary cross-entropy loss\n#     loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n#     return tf.reduce_mean(loss)\n\n\n\n# def binary_loss(y_true, y_pred):\n#     # Clipping y_pred to ensure it stays within (0, 1)\n#     y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n#     loss = -1 * y_true * K.log(y_pred)\n#     loss += -1 * (1 - y_true) * K.log(1 - y_pred)\n#     return K.mean(loss)\n\n## we used only binary_loss() for our experiment\ndef binary_loss(y_true, y_pred):\n  loss = -1 * y_true * K.log( K.clip(y_pred + K.epsilon(), 0, 1.0) )\n  loss += -1 * (1-y_true) * K.log( K.clip(1-y_pred+K.epsilon(), 0, 1.0) )\n  return loss\n\ndef weighted_mse(y_true, y_pred):\n    mse= tf.keras.losses.MeanSquaredError()\n    loss =mse(y_true, y_pred).numpy()\n    return loss\n\n\n## this is for a full token list with 239 tokens\n## these tokens are used to add with pre-trained tokenizers\n## after trained with the tokens, pretrained tokenizers can provide a full word tokenization and improvs expalinability\n\n\nnew_token_data = read_csv(full_token)     #(\"models/full_token_list.csv\")\nnew_token_list=new_token_data['words'].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.408894Z","iopub.execute_input":"2025-01-21T07:13:09.409096Z","iopub.status.idle":"2025-01-21T07:13:09.446527Z","shell.execute_reply.started":"2025-01-21T07:13:09.409079Z","shell.execute_reply":"2025-01-21T07:13:09.445965Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class ToxiSpanSE:\n    def __init__(self, ALGO=\"ROBERTA\", Tokenizer=\"roberta\",\n                 model_file=url, #\"models/CR_full_span_dataset.xlsx\",\n                 load_pretrained=False, max_len=70):\n        self.classifier_model = None\n        self.modelFile = model_file\n        self.Tokenizer=Tokenizer\n        self.ALGO = ALGO\n        self.data = self._read_data_from_file(model_file) # return full dataframe\n        self.load_pretrained = load_pretrained\n        self.max_length=max_len\n\n    def _read_data_from_file(self, model_file):\n        dataframe =pd.read_csv(model_file)\n        #dataframe=dataframe.iloc[:8]\n        dataframe.sample(frac=1).reset_index(drop=True) # call a random shuffle\n        #print(dataframe)\n        return  dataframe\n\n    ##get the DL models\n    def get_model(self, params, max_length):\n        ALGO = self.ALGO.upper()\n        training = params['training']\n        if(ALGO==\"BERT\"):\n            return get_bert(training, max_length)\n        elif(ALGO==\"DBERT\"):\n            return get_distilbert(training, max_length)\n        elif (ALGO == \"ALBERT\"):\n            return get_albert(training, max_length)\n        elif (ALGO == \"ROBERTA\"):\n            return get_roberta(training, max_length)\n        elif (ALGO == \"XLNET\"):\n            return get_xlnet(training, max_length)\n        else:\n            print(\"Unknown algorithm: \" + ALGO)\n            exit(1)\n\n    ##define tokenizer\n    def get_tokenizer(self):\n        tokenizer = tokenizer_collect(self.Tokenizer) #bert\n        return tokenizer #bert\n\n\n    def get_training_data(self):\n        return self.data  # full dataframe\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.447355Z","iopub.execute_input":"2025-01-21T07:13:09.447636Z","iopub.status.idle":"2025-01-21T07:13:09.454474Z","shell.execute_reply.started":"2025-01-21T07:13:09.447607Z","shell.execute_reply":"2025-01-21T07:13:09.453879Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport timeit\nimport pandas as pd\nfrom ast import literal_eval\n\n# Initialize parameters\nrand_state = 42\nmax_length = 70\nthreshold = None #.32\ntoxicClassifier = ToxiSpanSE()\n\n# Prepare dataset\ndataset = toxicClassifier.get_training_data() #dataframe\nif threshold:\n    threshold = float(threshold)\nrandom_folding = KFold(n_splits=6, shuffle=True, random_state=rand_state)\n\ndataset[\"spans\"] = dataset[\"spans\"].apply(literal_eval) #list [10, 11, 12, 13, 14, 15]\ntokenizer = toxicClassifier.get_tokenizer() # bert\n\n# Add custom tokens (optional)\n# tokenizer.add_tokens(new_token_list)\n\ndata_input, input_texts, data_output, initial_weights, tokens = sentence_preprocessing(\n    dataset, max_length, tokenizer     # dataframe,70,bert\n)\n# print(\"data output start\")\n# print(data_output)\n# print(\"data output end\")\n# Data Input: {'input_ids': array([[101, 2026, 2001, 2005, 1037, 3946, 102]]),\n#              'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]),\n#              'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}\n\n# Input Texts: ['yeah that sucked, fixed']\n\n# Data Output: [[0, 0, 0, 1, 0, 0, 0]]\n\n# Initial Weights: [[0, 1, 1, 1, 1, 1, 0]]\n\n# Tokens: {'input_ids': [[101, 2026, 2001, 2005, 1037, 3946, 102]],\n#          'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]],\n#          'attention_mask': [[1, 1, 1, 1, 1, 1, 1]],\n#          'special_tokens_mask': [[1, 0, 0, 0, 0, 0, 1]]\n#          \"offset_mapping\": [(0, 0), (0, 4), (5, 9), (10, 16), (16, 17), (18, 23), (0, 0)] \n#}\n\n# Debugging: Check the data shapes\nprint(f\"Data Input Shape: {data_input['input_ids'].shape}\") # (70,)\nprint(f\"Data Output Shape: {data_output.shape}\") # (1, 70)\n\n# Identify toxic words for validation\ntoxic_words = []\nfor i in range(data_output.shape[0]): # 1\n    for j in range(max_length): # 70\n        if data_output[i, j] == 1 and initial_weights[i, j] == 1:\n            toxic_words.append(tokenizer.convert_ids_to_tokens(tokens.input_ids[i][j])) # converted to string\n\n# Debugging toxic words (optional)\n# toxic_words_freq = pd.Series(toxic_words).value_counts()\n# print(toxic_words_freq[:15])\n\n# Start 10-fold CV\ncount = 1\nerror_analysis_output = []\nresults = \"\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:13:09.455182Z","iopub.execute_input":"2025-01-21T07:13:09.455426Z","iopub.status.idle":"2025-01-21T07:15:36.428945Z","shell.execute_reply.started":"2025-01-21T07:13:09.455394Z","shell.execute_reply":"2025-01-21T07:15:36.428297Z"}},"outputs":[{"name":"stdout","text":"Shape:  19651\n34600\nData Input Shape: (34600, 70)\nData Output Shape: (34600, 70)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def partition_input_dict(inputDict, dataIndex):\n    # print(\"dataindex start\")\n    # print(dataIndex)\n    # print(\"dataindex end\")\n    returnDict = dict()\n    returnDict[\"input_ids\"] = inputDict[\"input_ids\"][dataIndex]\n    returnDict[\"token_type_ids\"] = inputDict[\"token_type_ids\"][dataIndex]\n    returnDict[\"attention_mask\"] = inputDict[\"attention_mask\"][dataIndex]\n    #print(f\"returnDict[\"input_ids\"]:{returnDict[\"input_ids\"] }\")\n    # print(\"return dict start\")\n    # print(returnDict[\"input_ids\"])\n    # print(\"return dict end\")\n    return returnDict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:15:36.429755Z","iopub.execute_input":"2025-01-21T07:15:36.430021Z","iopub.status.idle":"2025-01-21T07:15:36.434300Z","shell.execute_reply.started":"2025-01-21T07:15:36.430000Z","shell.execute_reply":"2025-01-21T07:15:36.433628Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for train_index, test_index in random_folding.split(data_output): # Data Output: [[0, 0, 0, 1, 0, 0, 0]]    \n    start = timeit.default_timer()\n    print(f\"Using split-{count} as test data...\")\n\n    # print(\"train_index:{train_index}\")\n    # print(\"test_index:{test_index}\")\n    # print(\"data_output:{data_output}\")\n\n    # print(\"train_index start\")\n    # print(train_index)\n    # print(\"train_index end\")\n\n    # print(\"test_index start\")\n    # print(test_index)\n    # print(\"test_index end\")\n    \n    # Split data\n    train_input_set = partition_input_dict(data_input, train_index)\n    # print(\"train_input_set start\")\n    # print(train_input_set)\n    # print(\"train_input_set end\") \n    \n# Data Input: {'input_ids': array([[101, 2026, 2001, 2005, 1037, 3946, 102]]),\n#              'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0]]),\n#              'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}\n# train_index: [[0, 0, 0, 1, 0, 0, 0]]  \n    \n    test_input = partition_input_dict(data_input, test_index)\n    \n    test_texts = input_texts[test_index]\n    train_output = data_output[train_index]\n    test_output = data_output[test_index]\n\n    params = {\n        \"training\": True,\n        \"learning_rate\": 1e-5,\n        \"pre_training_epoch\": 0,\n        \"initial_epoch\": 0,\n        \"epochs\":30,\n        \"silver\": False,\n    }\n\n    training_index, validation_index = generate_test_train_index(train_output.shape[0], 0.1)\n    # print(\"training_index start\")\n    # print(training_index)\n    # print(\"training_index end\") \n\n    # print(\"validation_index start\")\n    # print(validation_index)\n    # print(\"validation_index end\") \n    \n    val_texts = input_texts[validation_index]\n    # print(\"val_texts start\")\n    # print(val_texts)\n    # print(\"val_texts end\") \n    model_training_input = partition_input_dict(train_input_set, training_index)\n    # print(\"model_training_input start\")\n    # print(model_training_input)\n    # print(\"model_training_input end\") \n    validation_input = partition_input_dict(train_input_set, validation_index)\n    # print(\"validation_input start\")\n    # print(validation_input)\n    # print(\"validation_input end\") \n    model_training_output = train_output[training_index]\n    # print(\"model_training_output start\")\n    # print(model_training_output)\n    # print(\"model_training_output end\") \n    validation_output = train_output[validation_index]\n    # print(\"validation_output start\")\n    # print(validation_output)\n    # print(\"validation_output end\") \n\n    # Build and compile the model\n    model = toxicClassifier.get_model(params, max_length)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"learning_rate\"])\n    model.compile(optimizer=optimizer, loss=binary_loss)\n    model.summary()\n\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n    sample_weight = initial_weights[training_index]\n\n    # Train the model\n    print(\"Training the model...\")\n    model.fit(\n        model_training_input,\n        model_training_output,\n        epochs=params[\"epochs\"],\n        initial_epoch=params['initial_epoch'],\n        validation_data=(validation_input, validation_output),\n        sample_weight=sample_weight,\n        callbacks=[es_callback],\n    )\n\n    # Debugging: Check training input/output shapes\n    print(f\"Training Input Shape: {model_training_input['input_ids'].shape}\")\n    print(f\"Validation Input Shape: {validation_input['input_ids'].shape}\")\n\n    # Predict on test data\n    test_pred = model.predict(test_input, verbose=1, batch_size=64)\n    \n    print(\"test_pred start\")\n    print(test_pred)\n    print(\"test_pred end\") \n    \n    stop = timeit.default_timer()\n    time_elapsed = stop - start\n    print(f\"Time elapsed for split-{count}: {time_elapsed} seconds\")\n\n    # Debugging: Inspect predictions\n    print(f\"Sample test predictions: {test_pred[:10]}\")\n    print(f\"Min prediction: {np.min(test_pred)}, Max prediction: {np.max(test_pred)}\")\n\n    \n    # Threshold tuning for validation set\n    if threshold is None:\n        val_pred = model.predict(validation_input, verbose=1, batch_size=64)\n        # print(\"val_pred start\")\n        # print(val_pred)\n        # print(\"val_pred end\")\n        print(f'Prediction shape: {val_pred.shape}')\n        best_threshold = 0.05\n        best_fscore = 0.0\n        for prob in np.arange(0.01, 0.99, 0.1):\n            prediction_label = classify_by_threshold(val_pred, prob)\n            # print(\"prediction_label start\")\n            # print(prediction_label)\n            # print(\"prediction_label end\") \n            avg_precision_0, avg_recall_0, avg_f1_0, avg_precision_1, avg_recall_1, avg_f1_1, _ = compute_performance(\n                validation_output, prediction_label, val_texts, tokenizer, max_length, False\n            )\n\n            results += f\"{count},{prob},{toxicClassifier.ALGO},{avg_precision_0},{avg_recall_0},{avg_f1_0},{avg_precision_1},{avg_recall_1},{avg_f1_1},{time_elapsed}\\n\"\n\n            ######################################################\n            # print(f\"Precision non-toxic: {avg_precision_0}\")\n            # print(f\"Recall non-toxic: {avg_recall_0}\")\n            # print(f\"F1 non-toxic: {avg_f1_0}\")\n            # print(f\"Precision toxic: {avg_precision_1}\")\n            # print(f\"Recall toxic: {avg_recall_1}\")\n            # print(f\"F1 toxic: {avg_f1_1}\")\n            ######################################################\n            \n            if avg_f1_1 > best_fscore:\n                best_threshold = prob\n                best_fscore = avg_f1_1\n            print(f\"Threshold: {prob}, F1 toxic: {avg_f1_1}\")\n\n        print(f\"Best F1 toxic: {best_fscore} at threshold: {best_threshold}\")\n\n    # Evaluate on the test set using a fixed threshold\n    else:\n        prediction_label = classify_by_threshold(test_pred, threshold)\n        avg_precision_0, avg_recall_0, avg_f1_0, avg_precision_1, avg_recall_1, avg_f1_1, retro_output = compute_performance(\n            test_output, prediction_label, test_texts, tokenizer, max_length, True\n        )\n        error_analysis_output.append(retro_output)\n\n        print(f\"Precision non-toxic: {avg_precision_0}\")\n        print(f\"Recall non-toxic: {avg_recall_0}\")\n        print(f\"F1 non-toxic: {avg_f1_0}\")\n        print(f\"Precision toxic: {avg_precision_1}\")\n        print(f\"Recall toxic: {avg_recall_1}\")\n        print(f\"F1 toxic: {avg_f1_1}\")\n\n        results += f\"{count},{threshold},{toxicClassifier.ALGO},{avg_precision_0},{avg_recall_0},{avg_f1_0},{avg_precision_1},{avg_recall_1},{avg_f1_1},{time_elapsed}\\n\"\n\n    count += 1\n\n# Debugging: Final output\nprint(\"Cross-validation completed.\")\nprint(\"Results Summary:\\n\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T07:15:36.435049Z","iopub.execute_input":"2025-01-21T07:15:36.435337Z","execution_failed":"2025-01-21T08:33:01.011Z"}},"outputs":[{"name":"stdout","text":"Using split-1 as test data...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'lm_head.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['tf_roberta_model.roberta.encoder.layer.0.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.0.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.0.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.0.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.0.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.0.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.0.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.0.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.0.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.0.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.0.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.0.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.0.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.0.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.1.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.1.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.1.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.1.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.1.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.1.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.1.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.1.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.1.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.2.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.2.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.2.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.2.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.2.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.2.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.2.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.2.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.2.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.3.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.3.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.3.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.3.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.3.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.3.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.3.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.3.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.3.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.4.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.4.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.4.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.4.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.4.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.4.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.4.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.4.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.4.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.5.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.5.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.5.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.5.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.5.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.5.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.5.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.5.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.5.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.6.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.6.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.6.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.6.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.6.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.6.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.6.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.6.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.6.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.7.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.7.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.7.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.7.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.7.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.7.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.7.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.7.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.7.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.8.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.8.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.8.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.8.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.8.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.8.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.8.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.8.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.8.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.9.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.9.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.9.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.9.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.9.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.9.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.9.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.9.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.9.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.10.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.10.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.10.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.10.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.10.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.10.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.10.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.10.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.10.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.query.weight', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.query.bias', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.key.weight', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.key.bias', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.value.weight', 'tf_roberta_model.roberta.encoder.layer.11.attention.self.value.bias', 'tf_roberta_model.roberta.encoder.layer.11.attention.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.11.attention.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'tf_roberta_model.roberta.encoder.layer.11.intermediate.dense.weight', 'tf_roberta_model.roberta.encoder.layer.11.intermediate.dense.bias', 'tf_roberta_model.roberta.encoder.layer.11.output.dense.weight', 'tf_roberta_model.roberta.encoder.layer.11.output.dense.bias', 'tf_roberta_model.roberta.encoder.layer.11.output.LayerNorm.weight', 'tf_roberta_model.roberta.encoder.layer.11.output.LayerNorm.bias', 'tf_roberta_model.roberta.pooler.dense.weight', 'tf_roberta_model.roberta.pooler.dense.bias', 'tf_roberta_model.roberta.embeddings.word_embeddings.weight', 'tf_roberta_model.roberta.embeddings.token_type_embeddings.weight', 'tf_roberta_model.roberta.embeddings.position_embeddings.weight', 'tf_roberta_model.roberta.embeddings.LayerNorm.weight', 'tf_roberta_model.roberta.embeddings.LayerNorm.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ token_type_ids            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_mask            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ my_layer (\u001b[38;5;33mMyLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ token_type_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                           │                        │                │ attention_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ token_type_ids            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_mask            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ my_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                           │                        │                │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Training the model...\nEpoch 1/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 228ms/step - loss: 0.1255 - val_loss: 0.0831\nEpoch 2/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - loss: 0.0239 - val_loss: 0.0383\nEpoch 3/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0142 - val_loss: 0.0266\nEpoch 4/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0121 - val_loss: 0.0221\nEpoch 5/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0115 - val_loss: 0.0200\nEpoch 6/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0113 - val_loss: 0.0188\nEpoch 7/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0111 - val_loss: 0.0181\nEpoch 8/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0110 - val_loss: 0.0175\nEpoch 9/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0110 - val_loss: 0.0170\nEpoch 10/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0108 - val_loss: 0.0165\nEpoch 11/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0107 - val_loss: 0.0163\nEpoch 12/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0107 - val_loss: 0.0161\nEpoch 13/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 173ms/step - loss: 0.0107 - val_loss: 0.0160\nEpoch 14/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 173ms/step - loss: 0.0106 - val_loss: 0.0159\nEpoch 15/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 173ms/step - loss: 0.0106 - val_loss: 0.0159\nEpoch 16/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 173ms/step - loss: 0.0106 - val_loss: 0.0157\nEpoch 17/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0106 - val_loss: 0.0157\nEpoch 18/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0106 - val_loss: 0.0158\nEpoch 19/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - loss: 0.0105 - val_loss: 0.0156\nEpoch 20/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - loss: 0.0106 - val_loss: 0.0156\nEpoch 21/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0156\nEpoch 22/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0157\nEpoch 23/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 176ms/step - loss: 0.0105 - val_loss: 0.0156\nEpoch 24/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0155\nEpoch 25/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0155\nEpoch 26/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0155\nEpoch 27/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0155\nEpoch 28/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0154\nEpoch 29/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - loss: 0.0105 - val_loss: 0.0155\nEpoch 30/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - loss: 0.0105 - val_loss: 0.0154\nTraining Input Shape: (25949, 70)\nValidation Input Shape: (2884, 70)\n\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 485ms/step\ntest_pred start\n[[1.08730830e-02 1.14088692e-02 4.45636874e-03 ... 2.53228663e-05\n  1.28713451e-04 1.10926259e-04]\n [8.59453715e-03 1.07655181e-02 1.88942589e-02 ... 6.84397673e-05\n  1.00265926e-04 3.96742136e-04]\n [3.51515114e-02 9.27785318e-03 3.60832289e-02 ... 6.69201254e-04\n  1.54110556e-03 3.39252962e-04]\n ...\n [1.22367740e-02 8.32896214e-03 1.04639595e-02 ... 5.73443307e-04\n  8.23282462e-04 2.45686620e-03]\n [1.01256315e-02 5.67769259e-02 3.82329971e-02 ... 3.01210501e-04\n  4.47900209e-04 2.15911004e-03]\n [2.13814601e-02 2.62737442e-02 7.22380495e-03 ... 2.18080007e-03\n  3.71031900e-04 5.23438852e-04]]\ntest_pred end\nTime elapsed for split-1: 4378.369207703 seconds\nSample test predictions: [[1.08730830e-02 1.14088692e-02 4.45636874e-03 7.18096038e-03\n  1.09734898e-02 5.27510233e-03 1.99837554e-02 1.36319036e-02\n  6.26709824e-03 6.73504220e-03 8.44317116e-03 4.52386448e-03\n  8.99500400e-03 1.69169754e-02 5.49692055e-03 3.70843783e-02\n  9.30309668e-03 1.20549034e-02 1.12243742e-02 4.66814125e-03\n  1.32226041e-02 1.06869806e-02 2.72119069e-03 6.63186875e-05\n  1.82833919e-05 1.15373485e-04 7.42278426e-05 1.07035659e-04\n  3.55775439e-04 4.25491948e-04 1.23652528e-04 3.06448434e-04\n  1.04850769e-04 2.85927352e-04 8.89250223e-05 1.24059152e-04\n  1.08458138e-04 2.92536599e-04 1.45904720e-04 5.63159047e-05\n  3.11729818e-04 1.03386148e-04 9.26230787e-05 8.51722652e-05\n  2.24524003e-04 2.81864952e-04 1.16833304e-04 5.24909468e-04\n  1.52507520e-04 5.04183117e-04 7.24079509e-05 6.88795626e-05\n  7.32995686e-05 4.77512833e-04 2.73245794e-04 5.07545628e-05\n  4.49853142e-05 1.59279021e-04 7.85818556e-05 3.88518820e-04\n  5.57028034e-05 1.47168263e-04 9.38020603e-05 5.03691190e-05\n  1.74925721e-04 1.87032230e-04 5.19558853e-05 2.53228663e-05\n  1.28713451e-04 1.10926259e-04]\n [8.59453715e-03 1.07655181e-02 1.88942589e-02 8.28466099e-03\n  9.09741223e-03 7.91688077e-03 7.79256830e-03 9.70276538e-03\n  9.44199692e-03 4.85048862e-03 2.90158740e-03 2.10122019e-02\n  1.31318523e-02 5.59889618e-03 1.01816691e-02 1.69924051e-02\n  3.31906625e-03 9.30049550e-03 6.73059095e-03 2.42113154e-02\n  3.72329280e-02 9.02436301e-03 1.14032322e-04 5.88030198e-05\n  3.12491429e-05 5.46045339e-05 4.58750255e-05 2.27856377e-04\n  1.04091465e-04 4.44523394e-05 8.04360243e-05 1.83268174e-04\n  2.32135062e-04 6.74928015e-05 1.27988169e-04 2.65429408e-04\n  2.62791204e-04 3.74393094e-05 1.33872411e-04 1.23709484e-04\n  6.33050950e-05 1.15367089e-04 2.76268780e-04 2.84910173e-04\n  1.92755891e-04 8.76071936e-05 3.03507666e-04 1.49559492e-04\n  1.21877063e-04 1.13596892e-04 7.83504103e-04 2.37724351e-04\n  9.61414014e-04 2.36306543e-04 1.30394998e-04 3.81320424e-05\n  9.25418572e-05 3.44192777e-05 1.23621998e-04 8.78290302e-05\n  2.21847687e-04 2.00648967e-04 7.19104501e-05 8.49224743e-05\n  1.10704874e-04 1.67300299e-04 2.24072555e-05 6.84397673e-05\n  1.00265926e-04 3.96742136e-04]\n [3.51515114e-02 9.27785318e-03 3.60832289e-02 8.91801529e-03\n  1.33511778e-02 2.25807112e-02 8.19353107e-03 2.41047796e-02\n  7.92678766e-05 9.67222513e-05 2.78239371e-04 2.64501519e-04\n  1.00935125e-04 5.87305636e-04 2.80775479e-04 3.23254295e-04\n  3.20929132e-04 1.53083893e-04 3.86601867e-04 9.63136044e-05\n  3.90674628e-04 1.25657505e-04 6.43296225e-04 1.86119782e-04\n  1.16923351e-04 7.43833662e-04 1.70266125e-04 2.31444646e-04\n  4.16866911e-04 2.72227044e-04 6.39011734e-04 3.07287177e-04\n  1.40232407e-03 4.51164175e-04 1.84893666e-04 9.11754149e-04\n  3.64119740e-04 7.64079392e-04 8.52510566e-04 2.67429510e-04\n  3.65874876e-04 6.70279769e-05 3.43671330e-04 1.56625450e-04\n  4.70929896e-04 1.24662402e-04 3.88382556e-04 3.58452031e-04\n  1.61158401e-04 5.91091171e-04 4.59072588e-04 2.61402573e-04\n  2.00321629e-05 8.55677645e-04 3.66055232e-04 6.21901243e-04\n  2.49924982e-04 3.25141533e-04 3.94000526e-04 9.25072818e-04\n  9.38749581e-04 1.29754117e-04 1.89514205e-04 1.04319130e-04\n  2.19475114e-04 6.07845606e-04 3.03396868e-04 6.69201254e-04\n  1.54110556e-03 3.39252962e-04]\n [6.16564509e-03 8.33003782e-03 1.06448559e-02 3.34642231e-02\n  9.76820476e-03 2.43049655e-02 1.17380666e-02 2.48912964e-02\n  7.74703920e-03 6.56011468e-03 1.26126334e-02 5.91070438e-03\n  9.85756144e-03 5.49555197e-03 3.92769352e-02 1.07617248e-02\n  5.94093697e-03 3.95255862e-03 1.17107779e-02 1.98785402e-02\n  1.69152176e-04 4.90977145e-05 8.22381990e-05 9.48982415e-05\n  5.15048450e-05 1.07260646e-04 2.58373446e-04 1.29855500e-04\n  1.08043183e-04 8.65786133e-05 2.84117588e-04 8.84293986e-05\n  5.69138792e-05 1.48963140e-04 1.54632377e-04 3.84546729e-05\n  1.27993786e-04 7.54081775e-05 6.04264860e-05 6.92504254e-05\n  3.58905585e-04 4.55052301e-04 2.58400774e-04 3.12290322e-05\n  2.23569994e-04 2.03335076e-04 3.25307105e-04 1.36558010e-04\n  2.59199995e-04 1.29842869e-04 3.27282731e-04 2.35775631e-04\n  4.35783768e-05 3.25491681e-04 2.36473556e-04 1.35638955e-04\n  3.34233657e-04 7.18031370e-05 1.26685918e-04 9.37477744e-05\n  1.35040274e-04 1.80409130e-04 1.56487236e-04 8.41913788e-05\n  2.66320367e-05 3.08571238e-04 2.35964792e-04 2.66524876e-04\n  4.10116423e-04 1.97448811e-04]\n [3.61913033e-02 3.68857682e-02 2.97558755e-02 4.90494333e-02\n  7.17754057e-03 4.05888781e-02 3.71627100e-02 2.09097692e-04\n  1.83058248e-04 1.77478243e-04 8.28300472e-05 9.44101121e-05\n  2.83246947e-04 1.83116732e-04 7.92884326e-04 2.66434188e-04\n  4.90576494e-04 1.08211522e-03 2.31397891e-04 4.80653369e-04\n  1.34836082e-04 4.01231926e-04 6.49384383e-05 6.89052162e-04\n  5.05664968e-04 1.29043285e-04 1.61385222e-04 4.73792199e-04\n  6.30921393e-04 5.22222370e-04 2.31936836e-04 7.71908322e-04\n  3.77533172e-04 4.84043005e-04 2.57832056e-04 2.11373321e-03\n  2.23945506e-04 2.61520594e-03 3.96738760e-04 3.88722226e-04\n  1.98942827e-04 5.12910658e-04 2.15639549e-04 4.76476591e-04\n  9.35224089e-05 3.20998603e-04 5.07103337e-04 1.52453576e-04\n  2.55133607e-04 3.89141002e-04 2.36235806e-04 2.89398504e-05\n  4.35481896e-04 4.09538392e-04 8.77092942e-04 6.63802610e-04\n  2.09505000e-04 5.02602255e-04 1.08133198e-03 4.55736037e-04\n  2.63600959e-04 7.56331603e-04 2.97836319e-04 7.44085992e-04\n  4.39076306e-04 5.14019222e-04 1.16937724e-03 1.47506839e-03\n  9.00390034e-04 5.17365872e-04]\n [5.88060841e-02 1.73151810e-02 9.22153816e-02 1.78987943e-02\n  2.35532671e-02 3.82096070e-04 1.63601755e-04 1.61903561e-04\n  2.44340656e-04 2.18637157e-04 4.92492283e-04 6.63219020e-04\n  1.84032004e-04 7.94352265e-04 1.90796432e-04 4.05535917e-04\n  2.26829143e-04 3.24437104e-04 5.48823853e-04 1.55866530e-03\n  1.24771454e-04 5.52176789e-04 9.43232328e-04 2.04134427e-04\n  5.05001459e-04 3.49058362e-04 1.84887656e-04 1.66570349e-03\n  1.43692858e-04 1.17493130e-03 6.88979635e-04 6.33902499e-04\n  6.65539294e-04 1.19048927e-03 2.85144168e-04 5.44462260e-03\n  1.44893944e-03 3.44537228e-04 2.94297875e-04 3.20406980e-04\n  1.05345651e-04 2.35546636e-03 2.20557136e-04 5.73890924e-04\n  1.23275362e-03 2.22799485e-04 1.07151840e-03 6.28619513e-04\n  1.35458924e-03 7.64483339e-05 9.81747056e-04 4.16625291e-04\n  1.35087245e-03 7.13913352e-04 2.11201783e-04 2.90457276e-04\n  6.43651583e-04 4.33374051e-04 4.01879486e-04 1.00488530e-03\n  3.51826457e-04 3.83551494e-04 8.40575260e-04 8.23277049e-04\n  2.47605518e-03 8.80180043e-04 4.67555947e-04 3.97958647e-04\n  3.53827985e-04 2.07057397e-04]\n [2.33883169e-02 1.22175030e-02 5.91161214e-02 3.26627158e-02\n  2.50171814e-02 3.74625772e-02 1.55786460e-04 1.31408917e-04\n  1.59244999e-04 1.79804309e-04 1.54999216e-04 1.05561849e-04\n  8.70694639e-04 2.57256004e-04 9.65441286e-04 6.00932573e-04\n  7.21044838e-04 6.63057494e-04 1.75135167e-04 4.59548755e-04\n  8.41480447e-04 2.58881657e-04 1.87763406e-04 7.80067465e-04\n  4.77980007e-04 8.54928730e-05 6.43797917e-04 3.36418976e-04\n  1.17840932e-03 2.31445301e-04 7.33585097e-04 5.57765772e-04\n  3.30976502e-04 3.62911320e-04 3.55879543e-03 2.63064459e-04\n  1.02057715e-03 1.29525433e-04 3.50502407e-04 1.80833988e-04\n  4.55323287e-04 2.73608690e-04 7.96592154e-04 2.75855273e-04\n  4.96873341e-04 9.56074393e-04 1.66394268e-04 6.34391443e-04\n  6.40787650e-04 8.83036642e-04 8.24411691e-05 4.63001546e-04\n  7.83871161e-04 1.39688456e-03 8.52037047e-04 1.18199002e-03\n  2.54368031e-04 9.23883694e-04 2.73213256e-04 1.45415426e-03\n  2.61074840e-03 1.83990953e-04 4.43249883e-04 6.36725919e-04\n  5.04105235e-04 1.52557413e-03 2.13347026e-03 8.18889705e-04\n  5.23472263e-04 6.22044900e-04]\n [1.46972649e-02 6.33567795e-02 2.12236512e-02 2.48593204e-02\n  1.20158615e-02 1.12839714e-02 5.18970340e-02 3.18321437e-02\n  2.27947105e-02 1.75065473e-02 3.59065772e-04 1.81553420e-04\n  2.04429118e-04 1.78737304e-04 5.68160576e-05 1.67006394e-04\n  3.85249237e-04 1.66831960e-04 1.67077917e-04 4.33145615e-04\n  2.32299572e-04 1.55676869e-04 1.47318444e-03 1.79386290e-04\n  6.04353612e-04 3.32643889e-04 4.04350983e-04 4.38336923e-04\n  1.34039103e-04 1.52081455e-04 4.35278926e-04 4.15691291e-04\n  3.59578407e-04 2.80615204e-04 6.18706981e-04 3.18416045e-04\n  1.10669622e-04 1.17434116e-04 1.26689021e-03 2.08106198e-04\n  4.60553216e-04 5.65380789e-04 4.56045207e-04 1.29477266e-04\n  5.23024180e-04 6.74980110e-05 8.23988521e-04 2.98090308e-04\n  3.22063861e-04 2.99484382e-04 2.60455970e-04 3.08608927e-04\n  6.49275724e-04 3.31545598e-04 7.70054976e-05 2.52798171e-04\n  5.46989730e-04 1.50443870e-03 8.20880348e-04 1.87667654e-04\n  2.79284955e-04 3.91652196e-04 2.28516699e-04 5.00903290e-04\n  5.01705566e-04 4.16002440e-04 4.10487031e-04 1.97682195e-04\n  9.12767253e-04 1.70613010e-03]\n [9.13046766e-03 1.34970499e-02 6.39664382e-03 2.80448832e-02\n  1.43190231e-02 1.45614687e-02 1.58678330e-02 1.86126716e-02\n  2.43500080e-02 2.87754964e-02 1.07570859e-02 2.19584014e-02\n  1.44240875e-02 1.10702915e-02 1.13109797e-02 9.93756403e-05\n  1.20435165e-04 1.01256439e-04 1.89592451e-04 7.20257449e-05\n  9.23728439e-05 1.86419944e-04 1.40007382e-04 2.59661814e-04\n  1.15726732e-04 6.05929061e-04 2.16273111e-04 2.08850615e-04\n  1.31159424e-04 3.78806115e-04 3.47556816e-05 1.06137486e-04\n  1.32812638e-04 3.33833232e-05 8.87852075e-05 2.12455823e-04\n  4.27691964e-04 2.43186936e-04 1.46741862e-04 4.01244208e-04\n  7.74125219e-04 1.69267209e-04 1.70023908e-04 2.66969961e-04\n  1.03512815e-04 7.42228411e-04 1.59670279e-04 1.62215234e-04\n  1.97845497e-04 1.87094294e-04 1.06460633e-04 3.77650664e-04\n  4.06411345e-05 3.54960153e-04 4.04046761e-04 1.98388778e-04\n  1.65019635e-04 3.73563729e-04 7.06780585e-04 1.36310700e-04\n  1.02405073e-04 2.31542217e-04 1.23560472e-04 1.99739618e-04\n  4.22980578e-04 1.44946112e-04 2.41499074e-04 3.48036760e-04\n  7.50987383e-05 5.04356110e-04]\n [1.19513134e-02 1.33909406e-02 7.80115230e-03 5.54525293e-03\n  6.62784511e-03 1.64942462e-02 1.96992960e-02 1.19476598e-02\n  1.02766119e-02 1.64010711e-02 1.15903392e-02 5.57152834e-03\n  1.53859444e-02 9.58178286e-03 9.40809958e-03 6.91131223e-03\n  2.63681971e-02 1.31648425e-02 1.23420337e-04 5.35341933e-05\n  4.40050862e-05 1.05972242e-04 8.53098027e-05 1.16725401e-04\n  2.50771613e-04 7.58295282e-05 6.28668931e-05 1.10924986e-04\n  2.23427429e-04 2.07160469e-04 1.02068640e-04 1.76576330e-04\n  3.91356414e-04 1.90682214e-04 1.27819381e-04 4.04811726e-04\n  2.02736672e-04 1.05415398e-04 2.61653680e-04 2.85274669e-04\n  4.63703793e-04 2.50734811e-05 3.05862894e-04 2.31258469e-04\n  3.60551727e-04 1.95443674e-04 3.04047804e-04 1.84057091e-04\n  3.93299561e-04 1.43043799e-04 7.66357771e-05 4.74581393e-05\n  4.72630898e-04 5.46812371e-05 1.62092008e-04 5.56056766e-05\n  1.83265380e-04 4.00833989e-04 1.57540780e-04 3.69142072e-04\n  2.07192861e-04 1.00153178e-04 1.10215085e-04 2.04449199e-04\n  1.41652519e-04 5.41584275e-04 2.51404563e-04 6.50951464e-04\n  1.32987901e-04 2.53700273e-04]]\nMin prediction: 2.246551048301626e-06, Max prediction: 0.14406457543373108\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 623ms/step\nPrediction shape: (2884, 70)\nThreshold: 0.01, F1 toxic: 0.02814615283979185\nThreshold: 0.11, F1 toxic: 0.0003236769703835572\nThreshold: 0.21000000000000002, F1 toxic: 0.0\nThreshold: 0.31000000000000005, F1 toxic: 0.0\nThreshold: 0.41000000000000003, F1 toxic: 0.0\nThreshold: 0.51, F1 toxic: 0.0\nThreshold: 0.6100000000000001, F1 toxic: 0.0\nThreshold: 0.7100000000000001, F1 toxic: 0.0\nThreshold: 0.81, F1 toxic: 0.0\nThreshold: 0.91, F1 toxic: 0.0\nBest F1 toxic: 0.02814615283979185 at threshold: 0.01\nUsing split-2 as test data...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'lm_head.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['tf_roberta_model_1.roberta.encoder.layer.0.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.0.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.0.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.1.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.1.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.2.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.2.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.3.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.3.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.4.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.4.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.5.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.5.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.6.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.6.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.7.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.7.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.8.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.8.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.9.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.9.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.10.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.10.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.query.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.query.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.key.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.key.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.value.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.self.value.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.intermediate.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.intermediate.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.output.dense.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.output.dense.bias', 'tf_roberta_model_1.roberta.encoder.layer.11.output.LayerNorm.weight', 'tf_roberta_model_1.roberta.encoder.layer.11.output.LayerNorm.bias', 'tf_roberta_model_1.roberta.pooler.dense.weight', 'tf_roberta_model_1.roberta.pooler.dense.bias', 'tf_roberta_model_1.roberta.embeddings.word_embeddings.weight', 'tf_roberta_model_1.roberta.embeddings.token_type_embeddings.weight', 'tf_roberta_model_1.roberta.embeddings.position_embeddings.weight', 'tf_roberta_model_1.roberta.embeddings.LayerNorm.weight', 'tf_roberta_model_1.roberta.embeddings.LayerNorm.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ token_type_ids            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_mask            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ my_layer_1 (\u001b[38;5;33mMyLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ token_type_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                           │                        │                │ attention_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ token_type_ids            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_mask            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ my_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                           │                        │                │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Training the model...\nEpoch 1/30\n\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 235ms/step - loss: 0.0372 - val_loss: 0.0658\nEpoch 2/30\n\u001b[1m105/811\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 162ms/step - loss: 0.0166","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# spans : [10, 11, 12, 13, 14, 15]\n\n# text : \"yeah that sucked, fixed\"\n\n#is_toxic : sucked\n\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-21T08:33:01.012Z"}},"outputs":[],"execution_count":null}]}